\chapter{Numerical methods for photonics}
\label{h:numeric}

\begin{quote}
The purpose of computing is insight, not numbers.

--- Richard Hamming
\end{quote}

\begin{quote}
Understanding grows only logarithmically with the number of floating point operations 

--- J.P. Boyd
\end{quote}

\chaptertoc


In this chapter, we will briefly touch upon a number of techniques to numerically solve the wave equation in cases where no analytic solution is available. Four different classes of methods will be introduced.

First, in \textbf{finite difference} methods, we will replace continuous derivatives with discrete approximations on a grid. When applying this to the wave equation in the frequency domain, this results in a linear system, which can be solved in a number of ways. Doing this discretisation in the time domain results in the popular finite-difference time-domain method (FDTD).

The \textbf{finite element} method does not directly solve a differential equation. Rather, it tackles the problem obliquely by converting it into a minimisation procedure. This so-called variational approach has advantages when it comes to e.g. sensitivity to rounding errors.

\textbf{Eigenmode expansion} methods are used in layered photonic structures, in which the fields are expanded in the basis set formed by the eigenmodes of each layer. Thanks to this choice of basis functions that incorporate a lot of the physics of the problem, this approach can be very efficient.

Finally, a more abstract framework is presented, namely the \textbf{method of the weighted residuals}. This framework can be used to generalise a number of individual approaches, like point matching or the Galkerkin method.

Each of these subjects could be the topic of an entire course in its own right, so we will only be able to introduce the basic principle of these methods, without going into any level of detail. What is important however, is to get a feeling for each of these methods, to get to know their strengths and weaknesses, so that you can make the correct choice of tool for any given problem.


\pagebreak

\sectionyoutubeugent{Finite-difference approximation of derivatives}{KEirr8vynUI}

Let's say we are not interested in the solution to a certain differential equation in the entire domain of interest, but only in a limited number of discrete points inside the simulation domain. The set of discretisation points is called the \textbf{mesh}, and the distance between discretisation points is called the \textbf{mesh size}. A typical choice of 2D mesh is a rectangular one, where you only consider points of the form $(i \Delta x, j \Delta y)$, with $i$ and $j$ integers.

\begin{cue}
What do you think are some advantages and disadvantages of having a small mesh size?  
\end{cue}

Obviously the hope is that as the mesh becomes finer and finer, the solution that we obtain converges more and more to the true solution of the wave equation. However, the price we pay for this consists of increased computational resources (time, memory, ...).

\begin{marginfigure}[-2.0cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Brook_Taylor
  \includegraphics{numeric/figures/b_taylor}
  \caption{Brook Taylor (1685-1731)}
\end{marginfigure}

Since we only have information about the solution at discrete points, we will need to approximate the derivatives that appear in the differential equation using data from those discrete points. To write down these so--called \textbf{finite-difference} approximations for derivatives, we will develop e.g. an unknown 1D function $f(x)$ in a Taylor series.

\begin{cue}
\noindent\marginnote[2.0cm]{The symbol $\mathcal{O}$ relates to the \textbf{order} of the remaining terms, i.e. how they scale as a function of $\Delta x$. Here, $\mathcal{O}\left(\Delta x^4\right)$ should really be interpreted as $\mathcal{O}\left((\Delta x)^4\right)$, i.e. if the grid size becomes twice as small, the sum of the remaining terms becomes roughly 16 times as small.}Write down Taylor expansions of $f(x+\Delta x)$ and $f(x-\Delta x)$ up till the third derivative. 
\end{cue}

\begin{gather}
f(x+\Delta x) = f(x) + \Delta x \frac{d f}{d x} + \frac{{(\Delta x)}^2}{2} \frac{d^2 f}{d x^2} + \frac{{(\Delta x)}^3}{6} \frac{d^3 f}{d x^3} + \mathcal{O}\left(\Delta x^4\right) \label{eq-taylor-plus} \\
f(x-\Delta x) = f(x) - \Delta x \frac{d f}{d x} + \frac{{(\Delta x)}^2}{2} \frac{d^2 f}{d x^2} - \frac{{(\Delta x)}^3}{6} \frac{d^3 f}{d x^3} + \mathcal{O}\left(\Delta x^4\right) \label{eq-taylor-min}
\end{gather} 

\begin{cue}
Neglecting all the terms including $\left(\Delta x\right)^2$ or higher, use Eq.~\ref{eq-taylor-plus} to approximate the first derivative of $f$. How does the error term scale with $\Delta x$? 
\end{cue}

From Eq.~\ref{eq-taylor-plus} we get

\begin{equation}
\frac{d f}{d x} = \frac{f(x+\Delta x) - f(x)}{\Delta x} + \mathcal{O}\left(\Delta x\right)
\end{equation} 

This is called the \textbf{forward difference} approximation of $df / dx$. Since the error term is $\mathcal{O}(\Delta x)$ (note that in deriving the equation above, we needed to divide by $\Delta x$), this is called a first order approximation.

\begin{cue}
Do the same, but start from Eq.~\ref{eq-taylor-min}.
\end{cue}

Similarly from Eq.~\ref{eq-taylor-min}:

\begin{equation}
\frac{d f}{d x} = \frac{f(x) - f(x- \Delta x)}{\Delta x} + \mathcal{O}\left(\Delta x\right)
\end{equation} 

This so--called \textbf{backward difference} is also first--order accurate.

\begin{cue}
Now subtract Eq.~\ref{eq-taylor-plus} and Eq.~\ref{eq-taylor-min}. What is the order of accuracy of the approximation of the derivative now? Can you explain this intuitively?
\end{cue}

By subtracting Eq.~\ref{eq-taylor-plus} and Eq.~\ref{eq-taylor-min}, we get a \textbf{central difference} formula, which now however is second--order accurate, because the lower--order terms have cancelled:

\begin{equation}
\fbox{$\displaystyle
\frac{d f}{d x} = \frac{f(x + \Delta x) - f(x- \Delta x)}{2 \Delta x} + \mathcal{O}\left(\Delta x^2\right)
$}
\end{equation} 

This formula is second--order accurate thanks to the cancellation of terms in the Taylor expansion. Another way of looking at it is that the central difference involves not one but both neighbouring points, so it encorporates more information on the local behaviour of the function, which makes it more accurate.

\begin{marginfigure}
\centering
\includegraphics{numeric/figures/fd}
\caption{The arc AB represents backward differences, BC forward differences and AC central differences.}
\label{fig-fd}
\end{marginfigure}

Forward, backward and central differences are illustrated in Fig.~\ref{fig-fd}. From this figure, you can clearly see that the central difference is more accurate.


\begin{cue}
Construct an approximation for the second derivative $f''$, this time adding Eq.~\ref{eq-taylor-plus} and Eq.~\ref{eq-taylor-min}. What is the order of approximation? 
\end{cue}

By adding Eq.~\ref{eq-taylor-plus} and Eq.~\ref{eq-taylor-min}, we get an approximation for the second derivative:

\begin{equation}
\fbox{$\displaystyle
\frac{d^2 f}{d x^2} = \frac{f(x + \Delta x) -2 f(x) + f(x - \Delta x)}{ \Delta x^2} + \mathcal{O}\left(\Delta x^2\right)
$}
\end{equation} 

If we want to get more accurate results, we could either use a finer mesh or add more information by including higher order neighbours like $f(x-2\Delta x)$ and $f(x+2\Delta x)$.

Another way of looking at the second derivative is by rewriting the approximation as

\begin{equation}
\frac{d^2 f}{d x^2} \sim  \frac{f(x + \Delta x) + f(x - \Delta x)}{2} - f(x)
\end{equation}

\noindent\marginnote{Drawing a function with an inflection point, where $f''=0$, allows you to verify that there the function is indeed equal to the average of its surroundings.}This shows that the second derivative (or the Laplacian for that matter) is all about the difference between the value of a function at a certain point and the average value of that function around that point.

\begin{marginfigure}[1.0cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/John_Crank
  \includegraphics{numeric/figures/j_crank}
  \caption{John Crank (1916-2006)}
\end{marginfigure}

\begin{marginfigure}[8.5cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Phyllis_Nicolson
  \includegraphics{numeric/figures/p_nicolson}
  \caption{Phyllis Nicolson (1917-1968)}
\end{marginfigure}

\begin{exer}
  % difficulty: normal
  % youtube: tU5OXUS6DxM
Construct a finite-difference scheme that will allow you to integrate the following differential equation:

$$\frac{du(t)}{dt} = -a u(t)$$

Calculate the approximations $u_n = u(n \Delta t)$ given an initial condition $u_0$ using several methods: \\

a) Approximate the time derivate using a forward difference. (This results in the forward Euler method.) \\

b) Same, but now use a backward difference. (This results in the backward Euler method.)\\

c) Construct a central difference at the intermediate time $t_{n+1/2}$ and eliminate $u_{n+1/2}$ by considering it to be the average of $u_{n}$ and $u_{n+1}$. (This results in the Crank-Nicholson method.) \\

d) Show that all of these methods can be formulated as one method (the $\theta$-rule):

$$u_{n+1} = \frac{1-(1-\theta) a \Delta t}{1+ \theta a \Delta t} u_n$$ \\

Which $\theta$-values correspond to which methods?

\end{exer}

\pagebreak

%\begin{exer}
% difficulty: normal
%Derive the following higher-order approximation of the second derivative:

%$$ \frac{d^2 f}{d x^2} = \frac{1}{12 \Delta x^2} \big[& -f(x + 2 \Delta x) + 16 f(x + \Delta x) -30 f(x) - f(x - 2 \Delta x) + 16 f(x- \Delta x) \big] + \mathcal{O}\left(\Delta x^4\right)$$

%\end{exer}


\begin{exer}
  % difficulty: normal
  % youtube: P6Fc_rOF_A0
Derive the following higher-order approximation of the second derivative:

\begin{align}  
  \frac{d^2 f}{d x^2} = \frac{1}{12 \Delta x^2} \big[& -f(x + 2 \Delta x) + 16 f(x + \Delta x) -30 f(x) \nonumber \\
  &  - f(x - 2 \Delta x) + 16 f(x- \Delta x) \big] + \mathcal{O}\left(\Delta x^4\right)
\end{align} 
\end{exer}


\pagebreak


\sectionyoutubeugent{Solving the 1D wave equation with finite differences}{TbIfp25tvQk}

Illustrating finite difference methods is best done using a simple example, so we will solve the 1D Helmholtz equation in a uniform medium:

\begin{equation}
\frac{d^2 f(x)}{d x^2} + k^2 f(x) = 0 \label{eq-helmholtz-1d}
\end{equation} 

We solve this problem at a fixed wavelength, i.e. a fixed value of $k$.

\begin{cue}
  Discretise this equation.
\end{cue}

The discretised version of Eq.~\ref{eq-helmholtz-1d} reads

\begin{equation}
\frac{f(x - \Delta x) -2 f(x) + f(x + \Delta x)}{ \Delta x^2} + k^2 f(x) = 0 \label{eq-hh-diff}
\end{equation}  
 
To make this example even more concrete, let us assume a uniform grid with $\Delta x=1$ from $x=1$ to $x=5$. To illustrate boundary conditions, let us further assume that the values of $f$ are fixed at the boundaries, e.g. $f(1)=f_1$ and $f(5)=f_5$. This could be from a source forcing the electric field to a constant value, or from a metal wall which forces the electric field to zero. 

We still need to determine the following unknowns: $f(2)$, $f(3)$ and $f(4)$.


\begin{cue}
Using the discretised equation and the boundary conditions, write down 5 expressions involving $f(1)$ through $f(5)$.
\end{cue}

The discretised version of Eq.~\ref{eq-helmholtz-1d}, including the boundary conditions, reads

\begin{align}
f(1)=& \, f_1 \\
f(1) -2f(2) + f(3) + k^2 f(2) =& \, 0 \\
f(2) -2f(3) + f(4) + k^2 f(3) =& \, 0\\
f(3) -2f(4) + f(5) + k^2 f(4) =& \, 0\\
f(5)=& \, f_5
\end{align} 

Using matrix notation, this becomes

\begin{gather}
\begin{bmatrix}
1& 0& 0& 0& 0 \\
1& k^2-2& 1& 0& 0 \\
0& 1& k^2-2& 1& 0  \\
0& 0& 1& k^2-2& 1  \\
0& 0& 0& 0& 1
\end{bmatrix}
\begin{bmatrix}
f(1) \\
f(2) \\
f(3) \\
f(4) \\
f(5)
\end{bmatrix}
= 
\begin{bmatrix}
f_1 \\
0 \\
0 \\
0 \\
f_5
\end{bmatrix} \label{eq-ex-fd}
\end{gather}

So, we've reduced the problem of solving a differential equation to the problem of solving a linear system of equations of the form ${\mathbf A}\cdot{\mathbf x}={\mathbf b}$.

It is important to realise that the discretisation step that we have introduced replaces the original equation with a new one, and that even an exact solution of the discretised problem will only yield an approximate solution of the original problem. The error we have introduced in this way, is called the \textbf{discretisation error}. The hope is that by refining the mesh size, we get successively smaller discretisation errors. 


\pagebreak


%\subsection{Solving the system ${\mathbf A} \cdot {\mathbf x}={\mathbf b}$}
\sectionyoutubeugent{Solving linear systems}{DetWn3j-P_c}


In order to solve the linear system ${\mathbf A}\cdot{\mathbf x}={\mathbf b}$ that arises from formulating a problem using finite differences, there are a number of options.

First of all, we can use a \textbf{direct method}, i.e. explicitly invert the matrix ${\mathbf A}$. Normally, this takes $O(N^3)$ operations, so it doesn't scale very well with large problem sizes. However, inspecting Eq.~\ref{eq-ex-fd}, we notice that the discretisation of the wave equation yields a tridiagonal matrix, where only three diagonals are non--zero. Matrix equations of this type can be solved quite efficiently, using a fairly compact variant of Gaussian elimination, taking only $O(N^2)$ operations.

A second philosophy is to give up the notion of solving the system exactly, and search for a faster approximate solution, converged relative to the discretisation error but perhaps not relative to machine round--off. This seems acceptable, especially since the algebraic equations are only an approximation to the continuum equations anyway, subject to discretisation error.

A way of constructing such an approximate solution is to use an \textbf{iterative method}. Let's start from an initial guess (e.g. setting all unknowns to zero, or to a random value, or to a known solution of a related problem). Then, construct successive approximations in the hope that after a sufficient number of iterations, the estimate converges to the true solution. Of course, this is only interesting if you manage to calculate successive approximations fairly quickly, and if you don't need a lot of iterations, so that the overall method is still efficient.

There exists a plethora of literature on iterative methods, which we obviously cannot treat in detail here, but in order to get a flavour for the basic ideas, we will now discuss one of the most basic iterative schemes (which unfortunately has limited practical use in its most simple incarnation).


\pagebreak

\section{Jacobi method}

\subsectionyoutube{The Jacobi method for the Helmholtz equation}{pWJDRvmL3eE}

\begin{marginfigure}[0.0cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi
  \includegraphics{numeric/figures/c_jacobi}
  \caption{Carl Gustav Jacob Jacobi (1804-1851)}
\end{marginfigure}

Let's return to the discretised version of the Helmholtz equation Eq.~\ref{eq-hh-diff}:

\begin{equation}
f_{i+1} -2 f_i + f_{i-1} + \Delta x^2 k^2 f_i = 0 \label{eq-hh-diff-2}
\end{equation}  

The subscript $i$ denotes the values at position $x_0 + i \Delta x$.

Now, let's start from an initial set of estimates $f_i^0$, where the superscript denotes the iteration number. For Jacobi iteration, the update rule to go from estimates $f_i^n$ to $ f_i^{n+1}$ is defined as

\begin{equation}
f_{i+1}^n -2 f_i^n + f_{i-1}^n + \Delta x^2 k^2 f_i^n = \alpha (f_i^{n+1} - f_i^n ) \label{eq-jacobi}
\end{equation}  

Here, $\alpha$ is by convention a positive real number. Another way of looking at Eq.~\ref{eq-jacobi} is saying that the difference between the old and the new field is proportional to the error (i.e. difference from zero) we get when plugging the old field in Eq.~\ref{eq-hh-diff-2}.

\begin{cue}
Have a look at this equation and try to understand why such an approach could work. What happens in case the method has found the true solution?  
\end{cue}

This can be understood as follows. If the method has converged, then the successive estimates won't change significantly anymore, such that $ f_i^{n+1} \thickapprox f_i^n $. But this means that the right--hand side of Eq.~\ref{eq-jacobi} will be equal to zero, or alternatively that $f_i^n$ is indeed the solution of Eq.~\ref{eq-hh-diff-2}.

The (positive) parameter $\alpha$ is called the relaxation parameter, which can be fine--tuned by the user of the method.

\begin{cue}
What could be advantages/disadvantages of having a small/large value of $\alpha$? 
\end{cue}

A small value of $\alpha$ means that the $f$ values will change rapidly from iteration to iteration, but with the risk of taking such large steps that the method won't converge. On the other hand, a larger value of $\alpha$ reduces the risk of diverging solutions, but on the other hand more iterations will be needed to reach convergence.

To clarify this further, we will now investigate when the Jacobi method diverges, i.e. when the $f$--values become unbounded for a bounded initial estimate.

\subsectionyoutube{Stability analysis of the Jacobi method}{HpHmTQILeCA}

Ideally, we'd like to know under what circumstances this method will converge to the exact solution. However, in this section, we're going to look at an easier subproblem: figuring out when the procedure will diverge, i.e. result in infinite field values. This stability is a necessary step for convergence.\marginnote{Turns out it's also a sufficient criterion, because of something called the Lax equivalence theorem.}

To study divergence, we take an approach that is based on Fourier analysis: the field profile at any iteration can be written as a sum of Fourier components $e^{-j k_x x}$, which are nothing other than plane waves with wavevector $k_x$. Let's take such a plane wave and investigate what happens when iterating through the Jacobi method. If the amplitude of a plane wave stays bounded when iterating, and this is true for all admissible wavevectors $k_x$, then we can be sure that whatever initial estimate will be used, the method will not produce an unbounded result at some time.

Our plane waves take the following form:

\begin{equation}
f_i^n = e^{-j k_x i \Delta x}
\end{equation} 

(Let's not worry about the amplitude, as we will be looking at the ratio $\frac{f_i^{n+1}}{f_i^{n}}$, and the Jacobi procedure is a linear one.)

\begin{cue}
Perform one Jacobi iteration on $f_i^n$, i.e. calculate $f_i^{n+1}$. Use that to calculate the ratio $\frac{f_i^{n+1}}{f_i^{n}}$, which will help us see whether this grows without bounds or not.  
\end{cue}

After one Jacobi iteration, we get:

\begin{align}
 e^{-j k_x (i+1) \Delta x} -2 e^{-j k_x i \Delta x} + 
  e^{-j k_x (i-1) \Delta x} + \Delta x^2 k^2  e^{-j k_x i \Delta x} \\ \nonumber
 = \alpha\left(f_i^{n+1} - e^{-j k_x i \Delta x} \right)
\end{align} 

or

\begin{align}
\alpha f_i^{n+1} -  \alpha e^{-j k_x i \Delta x} = e^{-j k_x i \Delta x}  \left[ e^{-j k_x \Delta x} -2 + e^{+j k_x \Delta x} + \Delta x^2 k^2 \right]
\end{align} 

Thanks to Euler, we can write that

\begin{align}
  \alpha f_i^{n+1}  =  e^{-j k_x i \Delta x}  \left[ \alpha +2 \cos {k_x \Delta x} -2 + \Delta x^2 k^2 \right]
\end{align} 

But $ e^{-j k_x i \Delta x}$ is $f_i^n$, so

\begin{equation}
\frac{f_i^{n+1}}{f_i^{n}} =  1 + \frac{1}{\alpha}\left[2 \cos ( k_x  \Delta x) - 2 + \Delta x^2 k^2\right]
\label{eq-jacobi-conv}
\end{equation} 

(That's actually the main reason we were studying the problem in the Fourier space: plane waves are eigenfunctions of the Helmholtz equation, so we can factor them out in our calculations here.)

\begin{cue}
Derive a criterion for this ratio to remain bounded, irrespective of the choice of a positive value of the relaxation parameter $\alpha$. Pay special attention to the DC plane wave component, i.e. $k_x=0$. 
\end{cue}

 In order for Eq.~\ref{eq-jacobi-conv} not to diverge, we should have that $\left|f_i^{n+1} / f_i^{n}\right| \le 1$. \marginnote{This is not sufficient, as $f_i^{n+1} / f_i^{n}$ should also be bigger than -1.}A necessary condition for this, is that the stuff between square brackets is negative (remember that by convention $\alpha$ is positive). So, at least we need to have that

\begin{equation}
2 \cos ( k_x  \Delta x) - 2 \le - \Delta x^2 k^2 \label{eq-conv-jacobi}
\end{equation} 

Since the cosine lies between -1 and 1, the left--hand side of this equation lies between -4 and 0 and reaches its maximum for $k_x=0$, which is an important case as it corresponds to a uniform solution. Since it's very likely that the final solution has a DC component, we better make sure that the method converges in this case. However, for this case, we can only satisfy Eq.~\ref{eq-conv-jacobi} if $k=0$. \marginnote{People have tried to address this using a pair of \emph{complex} values for $\alpha$ (J. Comp. Phys. 203, 358â€“370, 2005).}So, this means that unfortunately the Jacobi method is non--convergent for the general Helmholtz equation, but only works for Laplace's equation, i.e. the special case where $k=0$.

\pagebreak

\sectionyoutubeugent{Finite differences in the time domain}{Pmx2rbBBuOc}
\label{week6}

Without going into any detail, we finally want to mention extremely briefly the existence of a very popular method to solve Maxwell's equations in the time domain. This \textbf{finite--difference time--domain} method (FDTD) uses central differences to discretise the full vectorial Maxwell's equations both in space and time.

It uses a staggered grid, i.e. the electric field and the magnetic field are not discretised at the same point in space, but are arranged in a \textbf{Yee cell} (Fig.~\ref{fig-yee}). The arrangement of the field components in a Yee cell is such that the discretised curl laws of Maxwell's equations can be written out in a natural form.

\begin{figure}
\centering
\includegraphics[width=7cm]{numeric/figures/yeecell}
\caption{The Yee cell in the FDTD algorithm.}
\label{fig-yee}
\end{figure}

As for the time evolution of the fields, FDTD uses a leap--frogging scheme, where the electric fields calculated at $t=0$ are used to determine the magnetic fields at $t=0.5$. These are subsequently used to calculate the electric fields at $t=1$ and so on.

Such a brief description obviously glosses over many important points, so consult the literature for more details.

What is important to remember though, is that this is a very general method, but the price you pay for its brute-force character is a large computational cost, combined with intrinsic errors due to the discretisation. So, always do a convergence analysis of your results as a function of the grid size.

\pagebreak

\begin{exer}
% difficulty: normal  
Use Python and its libraries NumPy and SciPy to program a finite-difference method to solve the following differential equation

$$ -\left(a(x) u'(x) \right)' + c(x)u(x) = f(x)$$

in the interval $[0,1]$, with the functions $a$, $a'$, $c$ and $f$ given, and $u(0) = u(1) = 0$. Assume that $a$ and $c$ are positive in the interval of interest.\\

Expand the derivative of the product, and use a backward-difference approximation for $u'$ and a central-difference approximation for $u''$. \\

Use a sparse solver to take advantage of the banded matrix structure. Consult the SciPy documentation for the right way to construct these matrices. \\

To check your results, try the solver using the following set of functions:

$$a_1(x) = 1 + x^2$$
$$c_1(x) = 0$$
$$u_{\mathrm{sol}}(x) = x(1-x) e^x $$

To do so, use the differential equation to construct the required $f(x)$ from $a_1(x)$, $c_1(x)$ and the exact solution $u_{\mathrm{sol}}(x)$ and evaluate how close the numerical approximation of $u$ is to this analytical form.

\end{exer}



\pagebreak


\sectionyoutubeugent{Basic recipe for finite elements}{My6txtzMZHg}

Contrary to e.g. in finite differences, in finite elements we don't solve a differential equation directly, but rather take an oblique approach, as we will see below. All finite element methods more or less share the same philosophy, which is outlined in the following recipe:

\begin{itemize}
\item

\begin{marginfigure}
\centering
\includegraphics[width=3cm]{numeric/figures/staircase}
\caption{A triangular mesh follows a curved boundary much better than a square grid.}
\label{fig-staircasing}
\end{marginfigure}

\begin{marginfigure}
\centering
\includegraphics[scale=0.5]{numeric/figures/grid}
\caption{A triangular mesh that is locally refined.}
\label{fig-grid}
\end{marginfigure}

Subdivide the structure you want to model into $L$ finite subregions (the "finite elements"). A triangular mesh is a popular choice, because this allows one to approximate curved boundaries much better than e.g. with the rectangular grid that was used in finite difference methods (Fig.~\ref{fig-staircasing}). A second advantage is that all elements don't have to be the same size, so that you can use more elements where a higher resolution is required (Fig.~\ref{fig-grid}). Such a local refinement of the grid turns out to be much more complicated in a finite-difference method.
  
\item
In each separate element $l$, approximate the function $\psi$ you are looking for, using an approximation of the form
\begin{equation}
\psi_l(x,y) = \sum_{i=1}^M u_{i,l} b_{i,l}(x,y) \label{eq-fe-1}
\end{equation}

Here the $b_{i,l}(x,y)$ are some convenient set of known basis functions. In order to solve the problem, the $M$ unknown coefficients $u_{i,l}$ per element $l$ still need to be determined.

\item
Join the different elements, e.g. by ensuring that $\psi$ is continuous across the boundaries between the elements. This introduces some relationship between the $LM$ number of unknowns.


\item
Write down an expression $J$ involving the field in all elements, i.e. containing all the basis functions $b_{i,l}$ and all the expansion coefficients $u_{i,l}$. Because the  $b_{i,l}$ are chosen in advance and therefore known, $J$ is function of the unknown $u_{i,l}$ only. The precise form of $J$ differs from problem to problem, but is often related to the energy in the system.

\item
Find the coefficients $u_{i,l}$ such that $J$ is minimised. In a popular method due to \textbf{Rayleigh and Ritz}, this is done simply by setting all $\partial J / \partial u_{i,l}$ equal to zero and solving the resulting linear system. If $J$ is an expression for the total energy in the system, there is a clear physical interpretation for this procedure: the true solution to the problem is one that minimises the energy in the system.
\end{itemize}


\pagebreak


\sectionugent{Solving the 1D wave equation with finite elements}

\subsectionyoutube{Subdivide the structure}{wBzcZS_XkNo}

In order to make the recipe from the previous section more concrete, we will now indicate in more detail how finite elements can be used to solve the 1D Helmholtz equation in a uniform medium, i.e. the same problem we used to illustrate the concept of finite differences:

\begin{equation}
\frac{d^2 \psi (x)}{d x^2} + k^2 \psi(x) = 0 \label{eq-helmholtz-1d-2}
\end{equation} 

We will subdivide our domain into $L$ finite elements being line segments of identical length.

\subsectionyoutube{Approximate the unknown function}{ZGmyCwsTays}

To describe the unknown function $\psi$ in a certain element $l$, we will use a very simple but common approach, which is that of a piecewise straight approximation.  I.e., in each segment, we will assume that $\psi$ varies linearly between the end points (often called \textbf{nodes} of the mesh) $x_1$ and $x_2$ of that segment:

\noindent\marginnote[1cm]{To see that this is a linear interpolation, just substitute $x$ with $x_1$ and $x_2$ and check what happens.}

\begin{equation}
\psi_l(x) = \frac{x_2 - x}{x_2 - x_1} \psi_1 + \frac{x - x_1}{x_2 - x_1} \psi_2 \label{eq-fe-2}
\end{equation} 

Here $\psi_1$ and $\psi_2$ are the so far unknown values of $\psi$ at $x_1$ and $x_2$ respectively. (To lighten the notation, we omit the index $l$ here, but strictly speaking we should write $x_{1,l}$, $\psi_{2,l}$, etc.)

\begin{cue}
Looking at Eq.~\ref{eq-fe-2}, what would be the set of basis functions we use in this case?
\end{cue}

Comparing Eq.~\ref{eq-fe-1} and \ref{eq-fe-2}, we get that $M=2$, $u_i$ is $\psi_i$ and

\begin{align}
b_1(x) =& \, \frac{x_2 - x}{x_2 - x_1} \\
b_2(x) =& \, \frac{x - x_1}{x_2 - x_1}
\end{align} 

\begin{marginfigure}[-5cm]
\centering
\includegraphics{numeric/figures/interpol}
\caption{Simple shape functions $b_1(x)$ and $b_2(x)$ for a 1D finite element model.}
\label{fig-shape}
\end{marginfigure}

These shape functions or interpolation functions are sketched in Fig.~\ref{fig-shape}.

(It is sometimes useful to express the interpolation functions in terms of a local scaled coordinate $\xi=(x-x_1)/(x_2-x_1)$ ranging from 0 to 1. The advantage of local coordinates is that expressions can be more easily applied to the case where the size of the elements is not the same throughout the mesh.)

\subsectionyoutube{Join the elements}{5GLzU89pYc0}

As mentioned before, the elements are not independent, but are usually coupled through some continuity condition. In the case of our problem, $\psi$ needs to be continuous across elements. The effect of this is that the number of unknowns is reduced.

\begin{marginfigure}[0.5cm]
\centering
\includegraphics{numeric/figures/connect}
\caption{\textbf{Top}: disconnected local numbering. \textbf{Bottom}: connected global numbering.}
\label{fig-connect}
\end{marginfigure}

To see how this is done in practice, consider Fig.~\ref{fig-connect}. Rather than numbering the nodes $\psi_{l,1}$ and $\psi_{l,2}$, let's use a numbering scheme with only a single index. The top of Fig.~\ref{fig-connect} shows a \textbf{disconnected} local numbering scheme, where the two nodes of each element are always given a new number. On the other hand, the bottom of Fig.~\ref{fig-connect} shows a \textbf{connected} global numbering scheme, which already takes the continuity conditions into account. I.e., nodes with a different number in the disconnected scheme but which are physically the same, are merged and only given a single number.

\begin{cue}
For the example in Fig.~\ref{fig-connect}, use a matrix to express the relation between $\psi_{i, \mathrm{disc}}$ in the disconnected numbering scheme and  $\psi_{i, \mathrm{con}}$ in the connected numbering scheme.
\end{cue}
  
The disconnected values are related to connected values by the following matrix equation:

\begin{equation}
\begin{bmatrix}
\psi_1 \\ \psi_2 \\ \psi_3 \\ \psi_4 \\ \psi_5 \\ \psi_6
\end{bmatrix}_{\mathrm{disc}}
=
\begin{bmatrix}
1 & 0 & 0 & 0  \\ 
0 & 1 & 0 & 0  \\   
0 & 1 & 0 & 0  \\ 
0 & 0 & 1 & 0  \\   
0 & 0 & 1 & 0  \\  
0 & 0 & 0 & 1  
\end{bmatrix}
\begin{bmatrix}
\psi_1 \\ \psi_2 \\ \psi_3 \\ \psi_4
\end{bmatrix}_{\mathrm{con}}
\label{eq-con-matrix}
\end{equation} 

The matrix in Eq.~\ref{eq-con-matrix} is called the \textbf{connection matrix} $\mathbf C$ of the mesh. Obviously, for such a simple case it seems overkill to write the continuity equations in matrix form, but for complicated 3D meshes such a matrix has its practical value for a computer implementation.

\pagebreak

\subsectionyoutube{Write down an expression for $J$}{vdAuouW_9NY}

In order to keep the method physically interpretable, let's use the electromagnetic energy of the system as an expression for $J$.

From the complex Poynting theorem, we know that the time--averaged stored energy $\mathcal{E}$ in a system is given by (assume our systems are lossless and therefore have real $\varepsilon$ and $\mu$)

\begin{align}
\mathcal{E} =& \, \frac{1}{2} \int \left( {\mathbf E} \cdot {\mathbf D}^* - {\mathbf H} \cdot {\mathbf B}^* \right ) dx \\
  =& \, \frac{1}{2} \int \left( \varepsilon |{\mathbf E}|^2 - \mu |{\mathbf H}|^2 \right) dx
\end{align} 

\begin{cue}
Use Maxwell to eliminate ${\mathbf E}$ from that expression. Then, simplify it for the TM case where ${\mathbf H} = H {\mathbf 1}_z = \psi {\mathbf 1}_z $.  
\end{cue}

To eliminate ${\mathbf E}$, we can use one of Maxwell's curl laws:

\begin{equation}
{\mathbf E} = \frac{1}{j \omega \varepsilon} \nabla \times {\mathbf H}
\end{equation}  

Let's assume that we are working in the TM case, where ${\mathbf H} = H {\mathbf 1}_z = \psi {\mathbf 1}_z $. This means that in our 1D case, $\nabla \times {\mathbf H}=-{\frac{d H}{d x}}{\mathbf 1}_y $, so that we get

\begin{equation}
\mathcal{E} = \frac{1}{2} \int \left( \frac{1}{\omega ^2 \varepsilon}\left|{\frac{d H}{d x}}\right|^2 - \mu |H|^2 \right) dx
\end{equation}

Although we will not prove it here, it can be shown that in the case of lossless media, we can always scale the solution by a complex factor such that the absolute values in the previous equation can be replaced by simple squares.

Remember that we're working in a uniform medium, such that $\varepsilon$ is constant everywhere. That way, to minimise $\mathcal{E}$, we can just as well multiply everything by the constant $2 \omega ^2 \varepsilon$. Therefore, we can define $J$, our quantity to be minimised, as

\begin{equation}
J = \int \left( \left(\frac{d H}{d x}\right)^2 - k^2 H^2 \right) dx
\end{equation}

or, going back to our function $\psi$:

\begin{equation}
\fbox{$\displaystyle
J = \int \left( \left(\frac{d \psi}{d x}\right)^2 - k^2 \psi^2 \right) dx \label{eq-J-power}
$}
\end{equation} 

The exact form of $J$ can now be calculated in terms of the unknown nodal values $\psi_i$, using the explicit form of the shape functions from Eq.~\ref{eq-fe-2}. This is tedious but straightforward, and we will not bother with it here. However, we will just look at the general structure. In view of the linear nature of $\psi$ and the quadratic structure of Eq.~\ref{eq-J-power}, the contribution $J_l$ of a single element $l$ leads to an equation containing squares $\psi_i^2$ and mixed products $\psi_i \psi_j$ of the nodal values. This means we can write it as a bilinear form:

\begin{equation}
J_l = \begin{bmatrix}
\psi_{2l-1} \psi_{2l} 
\end{bmatrix}_\mathrm{disc}
\begin{bmatrix}
A & B \\
C & D 
\end{bmatrix}
\begin{bmatrix}
\psi_{2l-1} \\ \psi_{2l}
\end{bmatrix}_\mathrm{disc}
\label{eq-J-power-i}
\end{equation} 

The elements of the matrix are complicated expressions whose exact value can be determined from evaluating $J$ exactly. To be consistent with the numbering of the nodal values we used before, the index $l$ starts at 1, so that e.g. the nodal elements of the first element are $\psi_{1,\mathrm{disc}}$ and $\psi_{2,\mathrm{disc}}$.

Next up, the total energy $J$ is just the sum of the contributions $J_l$ from all elements. This total energy can also be cast in matrix form, this time with a block diagonal matrix, each block corresponding to a matrix of the type from Eq.~\ref{eq-J-power-i}.

\begin{exer}
  % difficulty: trivial
  % youtube: MLrrcDAmzzg 
Explicitly write down this total energy $J=\sum J_l$ as 

$$J={\mathbf \Psi}^{T}_\mathrm{disc} {\mathbf M} {\mathbf \Psi}_\mathrm{disc}
$$ 

Here, ${\mathbf M}$ is a block diagonal matrix, and ${\mathbf \Psi}_\mathrm{disc}$ is a column vector containing all nodal elements $\psi_i$ in the disconnected numbering scheme.
\end{exer}

Finally, the expressions in terms of disconnected nodal values can be transformed in terms of connected values using the connection matrix ${\mathbf C}$, like e.g. the one from Eq.~\ref{eq-con-matrix}. 

\begin{equation}
J = {\mathbf \Psi}^{T}_\mathrm{con}  {\mathbf C}^T {\mathbf M} {\mathbf C}  {\mathbf \Psi}_\mathrm{con}
\end{equation} 

In this way, all the continuity conditions are imposed.

\pagebreak

\subsectionyoutube{Minimise $J$}{tVEGbGtwvFU}

\begin{marginfigure}[0.5cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/John_William_Strutt,_3rd_Baron_Rayleigh
  \includegraphics{numeric/figures/Rayleigh}
  \caption{John William Strutt, 3rd Baron Rayleigh (1842-1919)}
\end{marginfigure}

To find the unknowns $\psi_{i,\mathrm{con}}$ that minimise $J$, we follow the Rayleigh-Ritz method and simply impose

\begin{equation}
\frac{\partial J}{\partial \psi_{i,\mathrm{con}}} = 0 \label{eq-rayleigh-ritz}
\end{equation} 

and this for all $i$. Remember that $J$ is a bilinear form, containing terms like $\psi_{i,\mathrm{con}}^2$ and $\psi_{i,\mathrm{con}}\psi_{j,\mathrm{con}}$. This means that Eq.~\ref{eq-rayleigh-ritz} takes the form of a set of linear equations, as many as there are unknowns. 

Just as was the case with the finite--difference method, this system can either be solved directly or iteratively.

If you're still uncomfortable that minimising $J$ is mathematically 100\% equivalent to solving the Helmholtz equation, don't worry, as we will show this more rigourously in a future section.

\pagebreak

\sectionyoutubeugent{Finite elements as a variational method}{klAmM-aV9rA}

In theory, the outline of the finite element method that we have given so far is perfectly adequate to describe it. However, it yields additional insight to cast it in the framework of a variational method.

Before doing that, let's recap some well-known facts, but in such a way that will make it easier for us to generalise to the new terminology.

A function $f(x)$ of one variable takes a number $x$ as input, and delivers another number as output. Suppose this function has a minimum for some value of $x$. At that point, the output of the function does not change to first order, i.e. $\delta f=0$ for all possible directions $\delta x$ around that point. Obviously, in this 1D case, there are only two trivial directions, i.e. $+x$ and $-x$.

A function $f(x,y)$ of two variables takes two numbers $x$ and $y$ as input, and spits out another number as output. Also here, when there is a minimum, the output of the function does not change, i.e. $\delta f=0$ for all possible directions  $\delta x \mathbf{1}_x  + \delta y\mathbf{1}_y$ you might explore in the neighbourhood of the minimum.

\begin{marginfigure}[-1.5cm]
  \includegraphics{numeric/figures/function_functional}
  \caption{Function vs. functional}
  \label{fig-function-functional}
\end{marginfigure}

Now, let's introduce a \textbf{functional}. This is an object $F(f(x))$ that consumes as input a function $f(x)$ and delivers a number as output (Fig.~\ref{fig-function-functional}). In contrast to functions, you could say that the input of a functional is not finite dimensional, but infinite dimensional, as it takes an infinite set of numbers to specify an arbitrary function. All of these objects however have a single number as output.

\begin{cue}
Can you think of examples of a functional?  
\end{cue}

The entity $J$ that we have introduced in this chapter is a functional, as it maps a function $\psi$ to a scalar (being in this case the energy of the system), so $\mathcal{E}=J(\psi)$.

\begin{exer}
  % difficulty: trivial
  % ugent
  % youtube: gbKEmkNIA6k
  Function or functional?

  a) $f(x)=\int_0^x t^2 dt$

  b) $f(x)=\int_0^1 x^2(t) dt$
  \begin{sol}
    a) function b) functional
  \end{sol}
\end{exer}

A \textbf{variational expression} comes from minimising a functional over a set of admissible functions. Indeed, every input $\psi$ to the functional will result in a different number $J(\psi)$ at the output, and we are looking for the special $\psi$ that will result in the smallest possible output.

In the case of finite elements, we have been solving the following variational expression:

\begin{equation}
\mathcal{E} = \min_\psi J(\psi)
\end{equation} 

where $\mathcal{E}$, is the total energy in the system, and $\psi$ are functions coming from an admissible set, e.g. continuous functions that satisfy the boundary conditions of the problem.

\begin{marginfigure}[-1.5cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Walther_Ritz
  \includegraphics{numeric/figures/w_ritz}
  \caption{Walther Ritz (1878-1909)}
\end{marginfigure}

The \textbf{Rayleigh--Ritz} method to minimise a functional is to expand $\psi$ in a finite number of known basis functions, and then determine the expansion coefficients such that the functional is minimised. This turns the infinite-dimensional problem into a much more tractable finite-dimensional one. (Nitpicking: this effectively turns the functional $J(\psi)$ into a vanilla function $J(\psi(\psi_i))$, albeit one of multiple arguments.)

\begin{cue}
What can you say about the sensitivity of $\mathcal{E}$, to e.g. rounding errors, given that we are working at the minimum of the functional? 
\end{cue}

\begin{marginfigure}[1.5cm]
  \includegraphics{numeric/figures/minimum}
  \caption{A function (or functional) does not vary to first order at its minimum.}
  \label{fig-functional-min}
\end{marginfigure}

So far, we don't seem to have done anything useful except introducing some new terminology. However, by looking at the energy $\mathcal{E}$, as a minimum of a functional, we realise that a small variation $\delta \psi$ (caused e.g. by numerical round--off errors) will not have a very big impact on the calculated value of $\mathcal{E}$, since $\delta J=0$ at the minimum of the functional, for all possible 'directions' $\delta \psi$. This insensitivity to errors is a big bonus for practical applications, and means that methods which have a variational basis can be much more robust.

Now, the energy $\mathcal{E}$ is a decidedly uninteresting quantity for practical applications, so the fact that we can calculate it without being very sensitive to rounding errors is of limited use. However, it is possible to derive variational expressions for e.g. the propagation constant of a waveguide, which is a much more relevant parameter that people are interested in.

\pagebreak


\sectionyoutubeugent{Variational form for the 3D Helmholtz equation}{vORxFwwLm6c}

In this section, we wish to provide some extra proof that a function that minimises the functional from Eq.~\ref{eq-J-power} is indeed a solution to the Helmholtz equation. So far, this was only made acceptable by invoking physical arguments based on minimum energy, but a mathematical proof for this was lacking. At the same time, let's extend the method to three dimensions:

\begin{equation}
J = \iiint \left((\nabla \psi)^2 - k^2 \psi^2 \right) dV \label{eq-variational1}
\end{equation} 

\noindent\marginnote{$\delta J=0$ can obviously also give us a maximum, but these situations are rare in practice and can be easily identified by exploring $J$ in the neighbourhood of the potential solution.}If a function $\psi$ minimises the functional $J$, then $\delta J$ has to be zero for $\psi$.

\begin{cue}
Calculate $\delta J = J(\psi + \delta \psi) - J(\psi)$ for this functional, neglecting second-order terms in $\delta \psi$.
\end{cue}

\noindent\marginnote[3.3cm]{Don't get confused: $\left( \nabla \psi \right)^2 = \nabla \psi \cdot \nabla \psi \ne \nabla^2 \psi = \nabla \cdot \nabla \psi$}For $\delta J$ (also called the \textbf{first variation} of $J$) we get

\begin{align}
\delta J =& \, J(\psi + \delta \psi) - J(\psi) \nonumber \\
  =& \, \iiint \left( (\nabla (\psi + \delta \psi))^2 - k^2 (\psi + \delta \psi)^2 \right) dV \nonumber   \\
  -& \, \iiint \left((\nabla \psi)^2 - k^2 \psi^2 \right) dV
\end{align} 

If we neglect second-order terms in $\delta \psi$, we get

\begin{equation}
\delta J = \iiint \left( 2 \nabla \psi \cdot \nabla (\delta \psi) - 2 k^2 \psi \delta \psi \right) dV \label{eq-var-helmholtz-1}
\end{equation}

Now, let's make use of the following vector identity:

\begin{equation}
\nabla \cdot (u \nabla v) = u \nabla \cdot \nabla v + (\nabla u) \cdot (\nabla v) \label{eq-vector-id-green-other}
\end{equation} 

\noindent\marginnote[-1.8cm]{To flex your vector calculus muscles, you can easily verify that all three terms turn out to be scalars.}

\begin{cue}
Take the volume integral of this identity and apply the divergence theorem. Next, use this expression to transform $\delta J$. Figure out how to best assign $u$, $v$ to $\psi$, $\delta \psi$, bearing in mind that the ultimate goal is to bring out the Helmholtz equation.
\end{cue}

Since the Helmholtz equation contains $\nabla^2 \psi = \nabla \cdot \nabla \psi$, let's assign $v$ to $\psi$ and $u$ to $\delta \psi$, as that will bring us closer to the Helmholtz equation. Additionally, this will result in a term that figures in Eq~\ref{eq-var-helmholtz-1}. By integrating Eq.~\ref{eq-vector-id-green-other}, we get

\begin{equation}
  \iiint  \nabla \cdot (\delta \psi \nabla \psi) dV =  \iiint \delta \psi  \nabla^2 \psi dV + 
  \iiint  \nabla (\delta \psi) \cdot \nabla \psi  dV  
\end{equation} 

Using the divergence theorem and bringing the final term, i.e. the term from Eq~\ref{eq-var-helmholtz-1}, to the left-hand side, we get

\begin{equation}
\iiint \nabla \psi \cdot \nabla (\delta \psi) dV = \iint \delta \psi {\nabla \psi} \cdot \mathbf{1}_n dS - \iiint \delta \psi \nabla^2 \psi dV
\end{equation} 

Here, $\mathbf{1}_n$ is the unit vector perpendicular to the surface, and ${\nabla \psi} \cdot \mathbf{1}_n $ is by definition equal to the normal derivative ${\partial \psi} / {\partial n}$. This means that

\begin{equation}
  \iiint \nabla \psi \cdot \nabla (\delta \psi) dV = \iint \delta \psi  \frac{\partial \psi}{\partial n} dS - \iiint \delta \psi \nabla^2 \psi dV
  \end{equation} 

\noindent\marginnote[-1.5cm]{This is actually one of the many incarnations of Green's theorem.}

\begin{cue}
Use this result in $\delta J$, and impose the condition that says that $\psi$ should minimise $J$.  
\end{cue}

Using this in the expression for $\delta J$, we get

\begin{equation}
\delta J = 2\iint \delta \psi  \frac{\partial \psi}{\partial n} dS - 2\iiint \delta \psi  \nabla^2 \psi dV - 2 \iiint k^2 \psi \delta \psi dV
\end{equation} 

Demanding that $\delta J = 0$ (a condition which is called \textbf{stationarity}), we get

\begin{equation}
\iiint \delta \psi (\nabla^2 \psi + k^2 \psi ) dV = \iint \delta \psi  \frac{\partial \psi}{\partial n} dS \label{eq-euler-lagrange-1}
\end{equation}

\begin{cue}
Think about how to deal with the term involving the boundary conditions, i.e. the 2D integral.  
\end{cue}

\pagebreak

\begin{marginfigure}[0.2cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Peter_Gustav_Lejeune_Dirichlet
  \includegraphics{numeric/figures/p_dirichlet}
  \caption{Peter Gustav Lejeune Dirichlet (1805-1859)}
\end{marginfigure}

Suppose that our boundary conditions are of the Dirichlet type such that $\psi=0$ on the boundary. In other words, we are restricting our search to only include functions that vanish at the boundary. Then, in order to be an admissible function that satisfies the boundary condition, $\delta \psi$ also has to be zero on the boundary, so that the right--hand side of Eq.~\ref{eq-euler-lagrange-1} will be zero.

Similarly, for Neumann--type boundary conditions where $\partial \psi / \partial n = 0$ on the boundary, the right--hand side of Eq.~\ref{eq-euler-lagrange-1} will also be zero.

So, for these common boundary conditions, we get that demanding that $\delta J = 0$ results in

\begin{equation}
\iiint \delta \psi (\nabla^2 \psi + k^2 \psi ) dV = 0
\end{equation} 

To minimise $J$, $\delta J$ should be zero not just for a particular lucky choice of $\delta \psi$, but it should actually be zero for any choice of $\delta \psi$. So, stationarity of the variational expression requires that $\psi$ has to satisfy

\begin{equation}
\nabla^2 \psi + k^2 \psi = 0 \label{eq-euler-lagrange-2}
\end{equation} 

This is of course the Helmholtz equation.

Eq.~\ref{eq-euler-lagrange-2} is called the \textbf{Euler} or the \textbf{Euler--Lagrange} equation of the variational formulation Eq.~\ref{eq-variational1}. So, rather than finding a function which minimises a certain functional in a variational formulation, we can explicitly solve a partial differential equation and achieve the same thing.

Other bits of terminology: a variational formulation of a problem is sometimes called a \emph{weak formulation}, whereas the corresponding differential equation is the \emph{strong formulation} of that problem.

\pagebreak

\begin{exer}
% difficulty: trivial
% ugent
% youtube: qM9NOykRESw
Show that demanding that the 1D Helmholtz equation is orthogonal to any arbitrary function leads to the variational form 

$$J = \int \left( \left(\frac{d \psi}{d x}\right)^2 - k^2 \psi^2 \right) dx$$

\end{exer}


\begin{exer}
  % difficulty: normal
  % youtube: 7Y6LC9GMrO8
  
Construct the best least-squares approximation to the parabola 

$$f(x) = 10(x-1)^2 -1$$ 

in the entire interval $[1,2]$ using only functions from $V = \mathrm{span} \{1, x\}$, i.e. the space of linear combinations of $1$ and $x$.

\begin{sol}
$$-\frac{38}{3} + 10x$$   
\end{sol}
\end{exer}


\begin{exer}
  % difficulty: normal
  % youtube: cwMI8XAVfCQ 
Use the identity

$$\iint_S p \nabla q \cdot \nabla r dS= - \iint_S q \nabla \cdot (p \nabla r) dS + \int_{\delta S} pq \frac{\partial r}{\partial n} dl$$

to find the Euler-Lagrange equation of the following variational expression

$$J(\phi)=\iint_S f(x,y) (\nabla \phi)^2 dS$$

with $f(x,y)$ a given scalar function.

\begin{sol}
$$\nabla \cdot \left( f \nabla \phi \right) = 0 $$
\end{sol}
\end{exer}

% Sorry Leonard, no space here.

%\begin{marginfigure}[3.0cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Leonhard_Euler
%  \includegraphics{numeric/figures/l_euler}
%  \caption{Leonhard Euler (1707-1783)}
%\end{marginfigure}

\begin{marginfigure}[-5.7cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Joseph-Louis_Lagrange 
  \includegraphics{numeric/figures/j_lagrange}
  \caption{Joseph-Louis Lagrange (1736-1813)}
\end{marginfigure}

\pagebreak

\begin{exer}
% difficulty: normal
% ugent
% youtube: cHTTlU_ewds
  
  Consider an arbitrarily shaped waveguide filled with air but bounded by perfectly conducting metal walls.

  a) Show that the eigenmodes of the waveguide, i.e. solutions which have a $z$-dependence of $\exp{(-j k_z z)}$, satisfy the following equation:

$$ \nabla_{xy}^2 \psi + \left( k^2 - k_z^2 \right)\psi = 0 $$ 

Here, $\nabla_{xy}$ is the transverse differential operator, which does not contain derivatives in the $z$-direction.

\noindent\marginnote{Note the difference between this cut-off condition for closed metal wave-guides and the one for open wave-guides, which is $k=k_{\mathrm{clad}}$} b) We are interested in finding the value of $k$ at cut--off ($k_z^2=0$), because we can get the cut-off wavelength from that. Show that the following is a variational expression for this cut--off wavevector $k_{\mathrm{cut}}$:

$$ k_{\mathrm{cut}}^2 = \frac{\iint_S (\nabla_{xy} \psi)^2 dS}{\iint_S \psi^2 dS}$$

Here $S$ is the cross--section of the waveguide.

c) Looking back at Eq.~\ref{eq-variational1}, what can you say about the energy of such a mode at cut-off?

\end{exer}


\pagebreak


\sectionugent{Eigenmode expansion}
\label{week7}

\subsectionyoutube{Introduction}{f2M9tZ8461o}

In this section, we'll discuss a method that is not as general as finite difference or finite element methods, but which, for a certain class of structures, can give an important speed boost because it exploits the properties of the structure to be modelled in a natural way.

In the so-called eigenmode expansion method, we are studying structures that consist of a number of layers where the refractive index profile does not change in the propagation direction (often taken to be the $z$--direction). \marginnote{If you're really desperate, you can even take a curved structure, and approximate it by slicing it up in a number of uniform layers, although in that case, you will suffer from staircasing.}This means that the structures are in essence a stack of waveguides, e.g. a Bragg reflector, or any structure that is like a layered cake. 
The simplest case is given in Fig.~\ref{fig-interface}, which shows the interface between two waveguides I and II. This is the building block of more complicated structures.

\begin{figure}[ht]
\centering
\includegraphics{numeric/figures/interface}
\caption{Interface between two layers.}
\label{fig-interface}
\end{figure}

We will expand the field in each waveguide using a basis set that incorporates already a lot of physics of the problem, namely the eigenmodes of that waveguide. It can be shown that these eigenmodes form a \textbf{complete} set, i.e. they can be used to expand any arbitrary field in that layer. Also, by enclosing the structure that we want to model in a metal box, this set of eigenmodes is a discrete set rather than a continuum.

The interface is placed at $z=0$ and a single mode with index $p$ is incident from medium I. This incident mode will give rise to a backward--propagating field in medium I, which we expand in terms of the eigenmodes of this medium. Likewise, we expand the transmitted field in the eigenmodes of medium II.

Before continuing, we need to determine a relationship between the fields of the forward and the backward propagating modes.

\begin{exer}
  % difficulty: trivial
  % youtube: W2p1YCQ6Ra0
Assume you solve Maxwell's equations to find a certain eigenmode solution, i.e. a solution with a $z$-dependence of $\exp{(-j \beta z)}$. Let's say this solution is characterised by the following tangential ($x$ and $y$) and normal ($z$) components:

$$\left( \mathbf{E}_{t},\mathbf{E}_{z},\mathbf{H}_{t},\mathbf{H}_{z},\beta \right)$$

By writing down the different components of Maxwell's curl equations, show that there also exists another solution:

$$  \left( \mathbf{E}_{t},-\mathbf{E}_{z},-\mathbf{H}_{t},\mathbf{H}_{z},-\beta \right) $$

Because of the negative propagation constant, this is the backward propagating version of the same eigenmode.

\end{exer}

\pagebreak

\subsectionyoutube{Mode matching}{q2mp-Y9VDxY}

Now that we've laid the groundwork, we can start applying the so-called \textbf{mode--matching} technique to the interface:

\begin{cue}
Impose the continuity of the tangential components of the total field on both sides of the interface in Fig.~\ref{fig-interface}.  
\end{cue}

Demanding continuity leads to: \marginnote[1.3cm]{Note that the expansion coefficients for $\mathbf{E}$ are the same as those for $\mathbf{H}$. This makes sense, because we're expanding in eigenmodes, and scaling each eigenmode will result in scaling both $\mathbf{E}$ and $\mathbf{H}$ with the same factor.} 

\begin{eqnarray}
\mathbf{E}^{I}_{p,t}+\sum _{j}R_{j,p}\mathbf{E}^{I}_{j,t} & = & \sum _{j}T_{j,p}\mathbf{E}^{II}_{j,t}\label{Eq:MM 1a} \\
\mathbf{H}^{I}_{p,t}-\sum _{j}R_{j,p}\mathbf{H}^{I}_{j,t} & = & \sum _{j}T_{j,p}\mathbf{H}^{II}_{j,t}\label{Eq:MM 1b} 
\end{eqnarray}

(The minus sign for the reflected tangential ${\mathbf H}$ field comes from the explicit form of the backward eigenmode we derived above.)

To calculate the unknown expansion coefficients $R_{j,p}$ and $T_{j,p}$ (which can be seen as reflection and transmission coefficients), we take the right cross product of Eq.~\ref{Eq:MM 1a} with $\mathbf{H}^{I}_{i,t}$ and the left cross product of Eq.~\ref{Eq:MM 1b} with $\mathbf{E}^{I}_{i,t}$. Here, $i$ is an arbitrary index. Afterwards, we integrate over the cross--section.

\begin{cue}
  Perform these operations.
\end{cue}

We get:

\begin{eqnarray}
\left\langle \mathbf{E}^{I}_{p},\mathbf{H}^{I}_{i}\right\rangle +\sum _{j}R_{j,p}\left\langle \mathbf{E}^{I}_{j},\mathbf{H}^{I}_{i}\right\rangle  & = & \sum _{j}T_{j,p}\left\langle \mathbf{E}^{II}_{j},\mathbf{H}^{I}_{i}\right\rangle \label{Eq:MM 2a} \\
\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{p}\right\rangle -\sum _{j}R_{j,p}\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{j}\right\rangle  & = & \sum _{j}T_{j,p}\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{II}_{j}\right\rangle \label{Eq:MM 2b} 
\end{eqnarray}

\noindent\marginnote{You can easily verify that in that definition, only the tangential components contribute to the integral.}where the scalar product is defined as the following overlap integral:

\begin{equation} 
\left\langle \mathbf{E}_{m},\mathbf{H}_{n}\right\rangle \equiv \iint _{s}\left( \mathbf{E}_{m}\times \mathbf{H}_{n}\right) \cdot d{\mathbf S}
\end{equation} 

If we decide to truncate the series expansion after $N$ terms, we have $2N$ unknowns: $N$ reflection coefficients and $N$ transmission coefficients. Eq.~\ref{Eq:MM 2a} and \ref{Eq:MM 2b} provide us exactly with $2N$ equations, since we can write them for all $i$ in $1 \cdots N$.

\subsectionyoutube{Exploiting orthogonality}{JdX-Ev-JK2I}

It's possible to show that eigenmodes in a waveguide satisfy the following orthogonality condition ($m \ne n$):

$$\iint_{S}\left( \mathbf{E}_{m}\times \mathbf{H}_{n}\right) \cdot d{\mathbf S}=0$$

\begin{cue}
Reduce the dimensionality of the linear system from the previous section by invoking this orthogonality relation, followed by adding and subtracting the resulting equations.
\end{cue}

The orthogonality condition leads to
  
\begin{eqnarray}
\delta _{ip}\left\langle \mathbf{E}^{I}_{p},\mathbf{H}^{I}_{p}\right\rangle +R_{i,p}\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{i}\right\rangle  & = & \sum _{j}T_{j,p}\left\langle \mathbf{E}^{II}_{j},\mathbf{H}^{I}_{i}\right\rangle \label{Eq:MM 3a} \\
\delta _{ip}\left\langle \mathbf{E}^{I}_{p},\mathbf{H}^{I}_{p}\right\rangle -R_{i,p}\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{i}\right\rangle  & = & \sum _{j}T_{j,p}\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{II}_{j}\right\rangle \label{Eq:MM 3b} 
\end{eqnarray}

Adding and subtracting these equations yields

\begin{eqnarray}
\sum _{j}\left[ \left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{II}_{j}\right\rangle +\left\langle \mathbf{E}^{II}_{j},\mathbf{H}^{I}_{i}\right\rangle \right] T_{j,p}=2\delta _{ip}\left\langle \mathbf{E}^{I}_{p},\mathbf{H}^{I}_{p}\right\rangle  &  & \label{Eq:MM 4a} \\
R_{i,p}=\frac{1}{2\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{i}\right\rangle }\sum _{j}\left[ \left\langle \mathbf{E}^{II}_{j},\mathbf{H}^{I}_{i}\right\rangle -\left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{II}_{j}\right\rangle \right] T_{j,p} &  & \label{Eq:MM 4b} 
\end{eqnarray}

This shows that we can first calculate the transmission coefficients by solving an $N \times N$ linear system, and then obtain the reflection coefficients by a simple matrix multiplication. This is much more efficient than solving the $2N \times 2N$ system from the previous section for the reflection and transmission coefficients simultaneously.

After obtaining $R$ and $T$ upon incidence of mode $p$, we can of course repeat the whole procedure using all modes $p$ in $ 1 \cdots N$.

\noindent\marginnote{Actually, when solving the system even an explicit inverse is not required, as we can use the LU decomposition of the system matrix.}Important to note is that this changes only the right--hand side in the linear system in Eq.~\ref{Eq:MM 4a}, so that we do not have to invert another system matrix. 

In what follows, we will also choose to normalise our modes such that \( \left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{I}_{i}\right\rangle =1 \), which further simplifies the equations.

\pagebreak

\subsectionyoutube{Overlap matrices}{pdeceZHUlOU}

Let's now define the following overlap matrices:

\begin{eqnarray}
\mathbf{O}_{I,II}\left( i,j\right)  & \equiv  & \left\langle \mathbf{E}^{I}_{i},\mathbf{H}^{II}_{j}\right\rangle \label{Eq:O_I_II} \\
\mathbf{O}_{II,I}\left( i,j\right)  & \equiv  & \left\langle \mathbf{E}^{II}_{i},\mathbf{H}^{I}_{j}\right\rangle \label{Eq:O_II_I} 
\end{eqnarray}

\begin{cue}
Use these matrices to write Eq.~\ref{Eq:MM 4a} and \ref{Eq:MM 4b} more compactly.   
\end{cue}

With the superscript $T$ denoting transpose, we finally get\marginnote{If you need more details here, watch the video.}:

\begin{eqnarray}
\mathbf{T}_{I,II} & = & 2\left( \mathbf{O}_{I,II}+\mathbf{O}^{T}_{II,I}\right) ^{-1}\label{Eq:MM 5a} \\
\mathbf{R}_{I,II} & = & \frac{1}{2}\left( \mathbf{O}^{T}_{II,I}-\mathbf{O}_{I,II}\right)  \mathbf{T}_{I,II}\label{Eq:MM 5b} 
\end{eqnarray}

In these expressions $\mathbf{T}_{I,II}$ and $\mathbf{R}_{I,II}$ are the so--called transmission and reflection matrices. Their \( p \)--th columns consist of the $T_{j,p}$ and $R_{j,p}$ from Eq.~\ref{Eq:MM 4a} and \ref{Eq:MM 4b}. 

If we collect the expansion coefficients of an arbitrary incident field in a column vector $\mathbf{A}_\mathrm{inc}$, we can write very compactly for the reflected and transmitted fields, using the meaning of the $R_{i,j}$ and $T_{i,j}$ coefficients: 

\begin{eqnarray}
\mathbf{A}_\textrm{refl} & = & \mathbf{R}_{I,II} \mathbf{A}_\textrm{inc}\label{Eq:R matrix} \\
\mathbf{A}_\textrm{trans} & = & \mathbf{T}_{I,II} \mathbf{A}_\textrm{inc}\label{Eq:T matrix} 
\end{eqnarray}

Obviously, we can repeat the entire procedure for incidence from medium II, which gives us the matrices $\mathbf{R}_{II,I}$ and $\mathbf{T}_{II,I}$.

These four matrices completely characterise the scattering that occurs at an interface.

\begin{cue}
Study the structure of Eq.~\ref{Eq:MM 5a} and \ref{Eq:MM 5b}. Do they remind you of something?  
\end{cue}

\begin{marginfigure}[-5.0cm]
  % credits: Wikipedia
  % url: https://en.wikipedia.org/wiki/Augustin-Jean_Fresnel
  \includegraphics{numeric/figures/a_fresnel}
  \caption{Augustin-Jean Fresnel (1788-1827)}
\end{marginfigure}

Finally, we want to point out the similarity in structure between Eq.~\ref{Eq:MM 5a} and \ref{Eq:MM 5b} and the well--known Fresnel equations to calculate reflection and transmission for normal incidence of a plane wave upon the interface between two semi--infinite media:

\begin{eqnarray}
T & = & \frac{2n_{1}}{n_{1}+n_{2}}\label{Eq:T normal} \\
R & = & \frac{n_{1}-n_{2}}{n_{1}+n_{2}}\label{Eq:R normal} 
\end{eqnarray}

In fact, Eq.~\ref{Eq:T normal} and \ref{Eq:R normal} can also be derived from the more general treatment presented here. For homogeneous layers, where the refractive index does not vary in the transverse direction, it turns out that the overlap matrices are diagonal, meaning that there is no cross--coupling between the different modes. Further evaluation of the formulas can reveal that the Fresnel equations are indeed recovered.

Obviously, a realistic structure will consist of more than one interface, but that problem is outside the scope of this text.


\begin{exer}
  % difficulty: hard
  % youtube: 6fDVdWvl09c
Derive the matrix form for the eigenmode expansion equations for the reflection and the transmission matrix at an interface, in case the basis functions are not orthogonal. Write them using a block matrix involving the overlap matrices.

\begin{sol}
$$ \begin{bmatrix} \mathbf{O}^{T}_{II,I} & -\mathbf{O}^{T}_{I,I} \\ \mathbf{O}_{I,II} & \mathbf{O}_{I,I} \end{bmatrix} \begin{bmatrix} \mathbf{T}_{I,II} \\ \mathbf{R}_{I,II} \end{bmatrix} =  \begin{bmatrix} \mathbf{O}^{T}_{I,I} \\ \mathbf{O}_{I,I} \end{bmatrix} $$
\end{sol}

\end{exer}


\pagebreak


\sectionugent{Method of weighted residuals}

To conclude this chapter, we will present the method of weighted residuals, a very general scheme for converting linear differential and integral equations into a form suitable for numerical solution by standard matrix methods.

\subsectionyoutube{Working with an error residual}{hdMlNCC6H8s}

We start from a problem of the type

\begin{equation}
L u = v \label{eq-operator}
\end{equation}

where $v$ represents some known excitation and $u$ is the unknown and wanted field. $L$ is a linear operator involving differentiation, integration or both.

\begin{cue}
Cast the Helmholtz equation in this form.  
\end{cue}

For the Helmholtz equation, $v=0$ and $L=\nabla^2 + k^2$. 

Suppose we approximate the unknown $u$ by

\begin{equation}
\tilde{u}(x) = \sum_{i=1}^N u_i b_i(x) \label{eq-expansion}
\end{equation} 

where $b_i(x)$ are a complete set of known basis functions. The general problem is now to choose the coefficients $u_i$ to approximate as well as possible the unknown solution $u$ to Eq.~\ref{eq-operator} with a \emph{finite} number $N$ of basis functions.

\begin{cue}
How would you quantify 'as well as possible'? Is the method you propose useful in practice?  
\end{cue}

Your first hunch could be to try and minimise the difference between the exact solution $u$ and its approximation $\tilde{u}$. However, this will get you nowhere in practice: you don't know the exact solution, so you cannot evaluate the difference.

\begin{cue}
What happens if you apply $L$ to the difference between $\tilde{u}$ and $u$? 
\end{cue}

Doing this, leads to the definition of the \textbf{error residual}:

\begin{equation}
R(x) = L\tilde{u} - Lu = L\tilde{u} - v = L \left( \sum_{i=1}^N u_i b_i(x) \right) - v(x)
\end{equation} 

which is clearly zero if and only if we have an exact solution to Eq.~\ref{eq-operator}. Moreover, apart from the coefficients $u_i$, all the other quantities can be calculated. Now we have a realistic objective: trying to make the residual as small as possible.

\begin{cue}
Think about how you would quantify and minimise the size of this residual. 
\end{cue}

There are many approaches you could take here, e.g. asking that the energy of the residual is minimised, or imposing that the residual vanishes at a set of predefined points.

Here however, we will introduce a more general approach, of which the examples listed above are special cases.

\noindent\marginnote{Don't confuse the weight functions $w_i$ with the weight function you could choose to define your scalar product, e.g. $r$ in the scalar product of Bessel functions. These are completely independent choices.}To proceed, we choose another set of functions, the so--called \textbf{test} or \textbf{weight} functions $w_j$. We also introduce an inner product (scalar product) formally written as $\langle f(x),g(x) \rangle$. The definition of the inner product is in a sense arbitrary and often follows from the specifics of the problem considered.

Rather than asking the impossible that $R(x)=0$ for all $x$, we insist that the residual be orthogonal to each of the weight functions under the scalar product considered (hence the name of the method).

\begin{cue}
Impose this condition and put it in matrix form.  
\end{cue}

This results in the following $N$ equations for $N$ unknowns:

\begin{equation}
\left\langle  L \left(\sum_{i=1}^N u_i b_i(x)\right) - v(x) , w_j(x) \right\rangle = 0 \hspace{0.5cm} j=1,2\cdots,N \label{eq-weighted-res}
\end{equation}

In the above equation, $b_i(x)$, $v(x)$ and $w_j(x)$ are known functions of $x$. $L$ is the known operator and $u_i$ are the unknown and wanted scalars which when put into Eq.~\ref{eq-expansion} give our approximate solution.

Let's move the excitation part to the right-hand side, and exploit the linearity of $L$ to write Eq.~\ref{eq-weighted-res} as

\begin{equation}
  \left\langle  \sum_{i=1}^N u_i L b_i(x), w_j(x) \right\rangle = \left\langle v(x), w_j(x)\right\rangle 
\end{equation}

Of course, the scalar product is linear as well:

\begin{equation}
   \sum_{i=1}^N u_i \left\langle L b_i(x), w_j(x) \right\rangle = \left\langle v(x), w_j(x)\right\rangle  \label{eq-weighted-res-3}
\end{equation}

This can also be put into a compact matrix form:

\begin{equation}
{\mathbf L} {\mathbf u} = {\mathbf v}
\end{equation} 

Here, $\mathbf u$ denotes the unknown column vector containing $u_1, u_2, \hdots, u_N$. $\mathbf v$ denotes the column vector with elements $\left\langle v(x), w_j(x) \right\rangle$. Finally, ${\mathbf L}$ is a  $N \times N$ matrix with $(i,j)$-th element $\left\langle L b_j(x), w_i(x) \right\rangle$. $\mathbf v$ and ${\mathbf L}$ are known, because we can evaluate them based on our choice of basis and test functions.

So, our equation $Lu=v$ has been \emph{projected} by approximation into the matrix equation ${\mathbf L} {\mathbf u} = {\mathbf v}$, which can be solved with standard numerical routines.

Apart from the weighted residuals method, the above procedure is also called the \textbf{method of moments} or the \textbf{generalised Galerkin method}.

\pagebreak

\subsectionyoutube{Choice of basis and test functions}{yzLwNFePkhk}

\begin{marginfigure}[-.0cm]
  % credits: Wikipedia
  % url:  https://en.wikipedia.org/wiki/Boris_Galerkin
  \includegraphics{numeric/figures/b_galerkin}
  \caption{Boris Grigoryevich Galerkin (1871-1945)}
\end{marginfigure}

Starting from the general procedure outlined above, different numerical methods can be derived that are catalogued according to their choice of basis and test functions.

If basis and test functions are chosen equal, i.e. $b_i(x) = w_i(x)$, the method is called the (non--generalised) \textbf{Galerkin method}. Often this leads to a formulation which can be cast in a variational form, having the advantages already outlined previously.

Another choice is

\begin{equation}
w_j(x) = \delta(x-x_j)
\end{equation} 

\begin{cue}
What will be the effect of choosing the test functions that way?  
\end{cue}

This corresponds to demanding the the error residual vanishes exactly at the points $x_j$ when the scalar product is defined as the integral of the product:

\begin{equation}
\left\langle R(x), w_j(x) \right\rangle = \int R(x) \delta(x-x_j) dx = R(x_j) = 0
\end{equation} 

This is called the \textbf{point matching} or \textbf{collocation method}.

Another interesting choice is explored in the following exercise:

\pagebreak

\begin{exer}
% difficulty: normal
% ugent
% youtube: ZdapTpUOIrE  
Assuming a commutative scalar product, show that the choice 

$$w_j(x) = L b_j(x)$$

leads to the minimisation of the norm of the error residual 

$$\left\langle R(x), R(x) \right\rangle$$ 

Because of this property, this choice is called the \textbf{least-squares residual} method.
\end{exer}

\pagebreak

\begin{exer}
  % difficulty: normal
  % youtube: yydWxzjMjUw
In the least-squares residual method, what is the geometrical interpretation of imposing that 

$$\left\langle R(x), w_j(x) \right\rangle = 0$$ 

for all values of $j$?
\end{exer}


\begin{exer}
  % difficulty: normal
  % youtube: hcwf0jV7Xd4
Construct an example where the Galerkin method and the least-squares residual method are identical.
\end{exer}

\pagebreak


\sectionugent{Comparison of different numerical techniques}

Finally, to wrap up this chapter, let's bring together what we've discussed about the most popular numerical methods to help you decide in which circumstances a certain method would be most appropriate.

\textbf{FDTD} is a very general method that can in theory be applied to a wide range of problems. However, because the structure is discretised on a square grid, it is not very well suited for curved boundaries because of the staircasing issue.  Another disadvantage of FDTD is that it is a brute-force method that is computationally expensive, requiring a lot of memory and CPU time.

\textbf{Finite element methods} don't suffer as much from staircasing because the edges of the elements can have an arbitrary orientation, not just horizontal and vertical. This makes them more suitable for curved structures. Also, if you expect your fields to varying rapidly in a certain region of your domain (e.g. inside a metal layer), then you can use smaller elements in that region and larger elements elsewhere. The flip side of that flexbility is that choosing the optimal mesh can be a bit of an art.

Finally, \textbf{eigenmode expansion} methods are a suitable choice for structures that can be modelled as a stack of waveguides. They are very efficient because they handle the propagation in the $z$-direction analytically, irrespective of the thickness of each layer. However, they are not as general as the other methods.

Finally, each numerical method has a set of metaparameters, like the grid size or the number of eigenmodes. It is important to not just perform a single simulation and call it a day. Rather, you should investigate what happens when you change these parameters, and how the results are affected. This \textbf{convergence analysis} will give you some confidence in knowing e.g. whether your grid is fine enough.

\pagebreak

\section*{Review questions}

\begin{itemize}
\item What are the advantages and disadvantages of the different numerical methods for photonics? In what circumstances would you use them?
\item What is the difference between forward, backward and central difference approximations to the derivative?
\item How does the finite-element method distinguish itself from other methods?
\item What is the Rayleigh-Ritz method?
\item What is a functional?
\item What is a variational problem? Stationarity? First variation?
\item What is the Euler-Lagrange equation of a variational problem?
\item For what problems is eigenmode expansion best used?
\item What are some common choices for test functions in the method of weighted residuals?  
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
